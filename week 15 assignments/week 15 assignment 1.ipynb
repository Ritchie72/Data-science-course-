{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21514e7c-fcbc-4e25-a0b5-10ecc385265d",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a43daa-9459-4a6b-af7c-3ab37be5117c",
   "metadata": {},
   "source": [
    "1] Simple linear regression models the relationship between a dependent variable (response variable) and a single independent variable (predictor variable). The model assumes a linear relationship between the two variables.\n",
    "Example : The weight of a person based on their height. Here, weight is the dependent variable, and height is the independent variable.\n",
    "\n",
    "2] Multiple linear regression models the relationship between a dependent variable and two or more independent variables. It assumes a linear relationship between the dependent variable and each of the independent variables.\n",
    "Example : Predicting the price of a house based on its size, number of bedrooms, and age. Here, the house price is the dependent variable, and the size, number of bedrooms, and age are the independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97766b9c-a04d-4694-8fb0-0fc1a1894203",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ba115-416b-4b09-a5ef-af4801fe33c2",
   "metadata": {},
   "source": [
    "1] Linearity: The relationship between the independent variables and the dependent variable is linear.\n",
    "\n",
    "2] Independence: The residuals (errors) are independent.\n",
    "\n",
    "3] Homoscedasticity: The residuals have constant variance at every level of the independent variables.\n",
    "\n",
    "4] Normality: The residuals of the model are normally distributed.\n",
    "\n",
    "5] No Multicollinearity (for multiple linear regression): Independent variables are not highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4d5d6-62c2-4ce1-8855-613f93a2161c",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34977320-6a23-4267-984c-bde34e853480",
   "metadata": {},
   "source": [
    "The slope represents the change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant in the case of multiple regression.\n",
    "\n",
    "The intercept is the value of the dependent variable when all independent variables are zero. It represents the baseline level of the dependent variable.\n",
    "\n",
    "Real-World Example: Predicting House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad8dc3-ffc6-40bb-aef5-6eeaa18fa096",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70377e07-3293-482d-8446-708d0c22946c",
   "metadata": {},
   "source": [
    "The goal of gradient descent is to find the values of the parameters (weights) of the model that minimize the cost function, which measures the difference between the predicted values and the actual values. The basic idea is to start with an initial set of parameter values and iteratively update them to reduce the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd83d2-f4bd-4a31-95f3-c4ae9b7be794",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd01d56-e54a-4cd6-b00a-2370071ac9f7",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression. It models the relationship between a dependent variable and two or more independent variables. This approach helps to understand how several factors influence the dependent variable simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06751b2d-4057-4392-875e-24beb5199ab0",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4d94f-9ce3-45c3-a0de-91139eac3db7",
   "metadata": {},
   "source": [
    "Multicollinearity in multiple linear regression occurs when independent variables are highly correlated, causing instability in the coefficient estimates, inflated standard errors, and reduced interpretability of the model. It can be detected using a correlation matrix, Variance Inflation Factor (VIF), tolerance values, and condition index. To address multicollinearity, one can remove highly correlated predictors, combine them into a single variable, use Principal Component Analysis (PCA) to create uncorrelated components, apply ridge regression which adds a penalty to the regression, or standardize the variables to reduce their correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e7a61-acfd-4ace-9bdc-783e0ca93d3e",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4caff-1aba-4d98-8175-01649619390b",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\) is modeled as an \\(n\\)-th degree polynomial. Unlike simple linear regression, which fits a straight line to the data, polynomial regression can fit a curve. The model takes the form \\( y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_n x^n ), allowing for more complex relationships between variables. This approach is particularly useful when the data exhibits a non-linear trend that a straight line cannot capture, providing more flexibility in capturing the patterns in the data compared to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71549bd-8580-4a1f-a06f-57b423465057",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443c8e0-7467-479a-ad54-7f674ed304ae",
   "metadata": {},
   "source": [
    "Polynomial regression offers greater flexibility than linear regression by fitting a curve to the data, which can capture non-linear relationships between the independent and dependent variables. This makes it advantageous when the data exhibits a non-linear trend that a simple straight line cannot adequately model. However, polynomial regression also has disadvantages: it is more prone to overfitting, especially with higher-degree polynomials, and can become computationally expensive. Additionally, the model can become complex and difficult to interpret. Polynomial regression is preferred when there is a clear non-linear relationship in the data that cannot be captured by linear regression, but caution is needed to avoid overfitting by selecting an appropriate polynomial degree and using techniques such as cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
